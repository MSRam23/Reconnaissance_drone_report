{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c561100-dc20-4951-8e74-f60dafe02462",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.qwen2_vl.processing_qwen2_vl because of the following error (look up to see its traceback):\nNo module named 'transformers.models.qwen2_vl.processing_qwen2_vl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.qwen2_vl.processing_qwen2_vl'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Example Usage:\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Initialize the processor and dataset\u001b[39;00m\n\u001b[0;32m     85\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-VL-2B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 86\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     87\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Qwen2VLDataset(data_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_dataset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Check the first sample in the dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:328\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AutoImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m--> 328\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    329\u001b[0m         )\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:141\u001b[0m, in \u001b[0;36mprocessor_class_from_name\u001b[1;34m(class_name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoProcessor is designed to be instantiated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing the `AutoProcessor.from_pretrained(pretrained_model_name_or_path)` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\lstr\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1969\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.qwen2_vl.processing_qwen2_vl because of the following error (look up to see its traceback):\nNo module named 'transformers.models.qwen2_vl.processing_qwen2_vl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class Qwen2VLDataset(Dataset):\n",
    "    def __init__(self, data_file, processor, image_size=(360, 420)):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        Args:\n",
    "        - data_file (str): Path to the JSON file with your dataset\n",
    "        - processor (AutoProcessor): HuggingFace processor for tokenizing\n",
    "        - image_size (tuple): Resize images to this size (default is (360, 420))\n",
    "        \"\"\"\n",
    "        with open(data_file, 'r') as f:\n",
    "            self.data = json.load(f)  # Load dataset from JSON file\n",
    "\n",
    "        self.processor = processor\n",
    "        self.image_size = image_size  # Resize image to fit within limits\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index for the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Tokenized input and labels\n",
    "        \"\"\"\n",
    "        item = self.data[idx]\n",
    "        messages = item['messages']\n",
    "        \n",
    "        # Extract user input (image/video + prompt)\n",
    "        user_message = messages[0]\n",
    "        content = user_message['content']\n",
    "        image_or_video = content[0]  # Image or video input\n",
    "        text_prompt = content[1]['text']  # The text prompt (question to describe the media)\n",
    "\n",
    "        # Process image/video input\n",
    "        if image_or_video['type'] == 'image':\n",
    "            image_path = image_or_video['image']\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = image.resize(self.image_size)  # Resize to match model limits\n",
    "            image_input = self.processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        elif image_or_video['type'] == 'video':\n",
    "            video_path = image_or_video['image']\n",
    "            # Handle video processing logic (optional)\n",
    "            # For simplicity, assume we're using one frame from the video or use a special processor\n",
    "            video_input = self.processor(video=video_path, return_tensors=\"pt\").pixel_values\n",
    "            image_input = video_input  # You can decide how to use the video frames\n",
    "            \n",
    "        # Prepare the text (description from assistant)\n",
    "        assistant_message = messages[1]\n",
    "        assistant_content = assistant_message['content']\n",
    "        text_description = assistant_content  # Target text\n",
    "\n",
    "        # Tokenize the inputs (text + image/video)\n",
    "        inputs = self.processor(\n",
    "            text=[text_prompt], \n",
    "            images=image_input, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Labels (text output)\n",
    "        labels = self.processor.batch_encode_plus(\n",
    "            [text_description], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        ).input_ids\n",
    "\n",
    "        # Move tensors to device (use GPU if available)\n",
    "        inputs = {key: value.squeeze().to(\"cuda\") for key, value in inputs.items()}\n",
    "        labels = labels.squeeze().to(\"cuda\")\n",
    "\n",
    "        return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': labels}\n",
    "\n",
    "# Example Usage:\n",
    "# Initialize the processor and dataset\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "train_dataset = Qwen2VLDataset(data_file=\"qwen2_dataset.json\", processor=processor)\n",
    "\n",
    "# Check the first sample in the dataset\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc2340-9b78-4eda-91e4-2b0a96b9c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2vl-lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=your_dataset,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba29b48c-200b-4cd6-b48d-90ecc241f986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'video',\n",
       "    'image': 'C:/Users/kmano/Downloads/nn.png',\n",
       "    'max_pixels': 151200,\n",
       "    'fps': 1.0},\n",
       "   {'type': 'text',\n",
       "    'text': 'Consider yourself as a airforce pilot who is operating a drone at this moment, explain this event.'}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b604151-af0e-4c49-816e-fd576a0f0dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an airforce pilot operating a drone, you are currently navigating a snowy urban environment. The drone is equipped with a traffic lights manager system, which is currently in the process of adjusting the traffic lights at a roundabout. The system is using a timed traffic lights manager to ensure that vehicles are moving smoothly and safely through the intersection.\n",
      "\n",
      "The roundabout is marked with a red circle, indicating that the traffic lights are currently in the red phase. The drone is monitoring the traffic lights and adjusting them accordingly to maintain order and prevent any accidents. The system is designed to adapt to changing traffic conditions and ensure that the roads are clear and safe for all vehicles.\n",
      "\n",
      "As an airforce pilot, your primary responsibility is to ensure the safety and efficiency of your drone operations. By monitoring the traffic lights and adjusting them as necessary, you are helping to maintain a smooth flow of traffic and prevent any accidents.\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"\".join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519927e5-8498-4330-9960-d823fad41e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an airforce pilot maneuvering a drone, you are currently at a crossroads with a traffic light that is currently red. The traffic light is positioned in the center of the intersection, and there are no vehicles or pedestrians visible in the immediate vicinity. The traffic light is likely controlling the flow of traffic at this intersection, ensuring that vehicles and pedestrians can safely navigate the intersection.\n",
      "\n",
      "In this scenario, you would need to monitor the traffic light closely and adjust your drone's position accordingly to maintain a safe distance from the traffic light. You would need to ensure that your drone is not obstructing the traffic light or causing any delays for vehicles and pedestrians. Additionally, you would need to be aware of any changes in the traffic light's signal, such as a change to green or yellow, and adjust your drone's position accordingly to maintain a safe distance from the intersection.\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a928b6a-900f-4f0b-8900-1b45ffb595d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb7b79-7d75-425f-b95f-ce810b457bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
